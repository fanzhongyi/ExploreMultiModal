defaults:
  - _self_
  - model: vlmo_debug
  - train: pretrain_mum
  - ds_stage: l1
  - override hydra/job_logging: none
  - override hydra/hydra_logging: none
  # - override hydra/sweeper: nevergrad

hydra:
  run:
    dir: ./


data:
  # Path to dataset
  data_root: 'datasets/arrows/'
  # Batchsize per GPU
  batch_size: 256
  eval_batch_size: ~
  # Only get image
  image_only: false
  # Input image size
  img_size: ${model.img_size}

  # Mask Image Nodeling needed
  patch_size: ${model.patch_size}
  num_mask_patches: 75
  max_mask_patches_per_block: ~
  min_mask_patches_per_block: 16

  # Tokenizer
  tokenizer: 'bert-base-uncased'
  # Do whole mask
  whole_word_masking: true
  # MLM Probability
  mlm_prob: 0.15

  # Dataloader num_workers
  py_num_workers: 10
  # CPU preload
  prefetch_factor: 10

  # Background Dataloader
  bg_loader: true
  # GPU preload
  prefetch_queue_depth: 1

  # VQA answer table size
  vqav2_label_size: 3129


dist:
  distributed: true
  # torchrun config by ENV
  master_addr : ~
  master_port : ~
  rank : ~
  world_size : ~
  local_rank : ~
  local_world_size : ~
  dist_backend: 'nccl'
  dist_url: 'env://'
  # Slurm ENV only config by ENV
  slurm_enable : false
  slurm_job_id : ~
  slurm_procid : ~
  slurm_ntasks : ~
  slurm_job_num_nodes : ~
  slurm_ntasks_per_node : ~
  slurm_nodelist : ~
  slurm_nodename : ''


# deepspeed config by CMD
deepspeed:
  enable: false
  # torch state_dict init for deepspeed
  pth2ds: ~
  # ds_config to ds.init
  config:
    # train_batch_size: ${data.batch_size}
    train_micro_batch_size_per_gpu: ${data.batch_size}
    gradient_accumulation_steps: 1
    steps_per_print: 3000
    optimizer:
      type: Adam
      adam_w_mode: True
      params:
        lr: ${train.base_lr}
        weight_decay: ${train.weight_decay}
        bias_correction: true
        betas: ${train.opt.betas}
        eps: 1e-8
    fp16 :
      enabled: true
      loss_scale: 0
      initial_scale_power: 7
      loss_scale_window: 128
    gradient_clipping: ${train.clip_grad}
    zero_optimization: ${ds_stage}


wandb:
  enable: true
  # host: localhost
  # token: local-b114b776a0636f41281c6c36bfdf77a2dc115022
  # mode: online
  host: api.wandb.ai
  token: 30e859c562557e3cb316b5863156a37c09569611
  mode: offline
  project: vlmo
  name: "${tag}-${dist.slurm_job_id}"
  id: "${train.phase}-${tag}-${dist.slurm_job_id}"
  alert: false


# Debug flag for ipdb
ipdb : false
# Experiment Group Folder
exp_dir: ~
# Output : Experiment / Time
output_dir : 'output'
# Tag of experiment
tag : 'default'
# Fixed random seed
seed : 0
# Perform evaluation only
eval_mode : false
# Test throughput only
throughput_mode : false
# Logger level
log_level: 'debug'

# VLMO momentum update
vlmo_ema: false
vlmo_ema_decay: 0.995

# EMA
model_ema: false
model_ema_decay: 0.9999

# Final_metrics for tuning hyperparameters
minimize_metric: ~

object_handle:
  logger_handle: ~
  id2ans: ~
  ans2id: ~
  vqa_dict: ~
