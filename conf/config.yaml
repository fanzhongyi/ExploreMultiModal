defaults:
  - _self_
  - model: vlmo_debug
  - train: pretrain_mum
  - override hydra/job_logging: none
  - override hydra/hydra_logging: none
  # - override hydra/sweeper: nevergrad

hydra:
  run:
    dir: ./


data:
  # Path to dataset
  data_root: 'datasets/arrows/'
  # Batchsize per GPU
  batch_size: 256
  eval_batch_size: ~
  # Only get image
  image_only: false
  # Input image size
  img_size: ${model.img_size}

  # Tokenizer
  tokenizer: 'bert-base-uncased'
  # Do whole mask
  whole_word_masking: true
  # MLM Probability
  mlm_prob: 0.15

  # Dataloader num_workers
  py_num_workers: 10
  # CPU preload
  prefetch_factor: 10

  # Background Dataloader
  bg_loader: true
  # GPU preload
  prefetch_queue_depth: 1

  # VQA answer table size
  vqav2_label_size: 3129


dist:
  distributed: true
  # torchrun config by ENV
  master_addr : ~
  master_port : ~
  rank : ~
  world_size : ~
  local_rank : ~
  local_world_size : ~
  dist_backend: 'nccl'
  dist_url: 'env://'
  # Slurm ENV only config by ENV
  slurm_enable : false
  slurm_job_id : ~
  slurm_procid : ~
  slurm_ntasks : ~
  slurm_job_num_nodes : ~
  slurm_ntasks_per_node : ~
  slurm_nodelist : ~
  slurm_nodename : ''


# deepspeed config by CMD
deepspeed:
  enable: false
  # torch state_dict init for deepspeed
  pth2ds: ~
  # ds_config to ds.init
  config:
    # train_batch_size: ${data.batch_size}
    train_micro_batch_size_per_gpu: ${data.batch_size}
    gradient_accumulation_steps: 1
    steps_per_print: 3000
    optimizer:
      type: Adam
      adam_w_mode: True
      params:
        lr: ${train.base_lr}
        weight_decay: ${train.weight_decay}
        bias_correction: true
        betas: ${train.opt.betas}
        eps: 1e-8
    fp16 :
      enabled: true
      loss_scale: 0
      initial_scale_power: 7
      loss_scale_window: 128
    gradient_clipping: ${train.clip_grad}
    zero_optimization: ${deepspeed.optimize_level.stage_3}
  # deepspeed zero optimization level
  optimize_level:
    stage_1:
      stage: 1
      reduce_bucket_size: 5e8
    stage_2:
      stage: 2
      contiguous_gradients: true
      overlap_comm: true
      reduce_scatter: true
      reduce_bucket_size: 5e8
      allgather_bucket_size: 5e8
    stage_3:
      stage: 3
      contiguous_gradients: true
      overlap_comm: true
      reduce_scatter: true
      reduce_bucket_size: 1e7
      stage3_max_live_parameters: 1e9
      stage3_max_reuse_distance: 1e9
      stage3_prefetch_bucket_size: 1e7
      stage3_param_persistence_threshold: 1e5
      sub_group_size: 1e9
      offload_optimizer:
        device: cpu
        pin_memory: true
      offload_param:
        device: cpu
        pin_memory: true


wandb:
  enable: true
  # host: localhost
  # token: local-b114b776a0636f41281c6c36bfdf77a2dc115022
  # mode: online
  host: api.wandb.ai
  token: 30e859c562557e3cb316b5863156a37c09569611
  mode: offline
  project: vlmo
  name: "${train.phase}-${tag}-${dist.slurm_job_id}"
  alert: false


# Debug flag for ipdb
ipdb : false
# Experiment Group Folder
exp_dir: ~
# Output : Experiment / Time
output_dir : 'output'
# Tag of experiment
tag : 'default'
# Fixed random seed
seed : 0
# Perform evaluation only
eval_mode : false
# Test throughput only
throughput_mode : false
# Logger level
log_level: 'debug'

# VLMO momentum update
vlmo_ema: false
vlmo_ema_decay: 0.9999

# EMA
model_ema: false
model_ema_decay: 0.9999

# Final_metrics for tuning hyperparameters
minimize_metric: ~

object_handle:
  logger_handle: ~
  id2ans: ~
  ans2id: ~
  vqa_dict: ~
