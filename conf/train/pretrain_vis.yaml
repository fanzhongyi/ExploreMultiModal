train_phase: 'pretrain_vis'




start_epoch : 0
epochs : 4
warmup_epochs : 2
weight_decay : 0.1
base_lr : 1e-4
warmup_lr : 5e-7
min_lr : 5E-6

# Clip gradient norm
clip_grad : 5.0
# Auto resume from latest checkpoint
auto_resume : false
# Gradient accumulation steps
accumulation_steps : 0
# Whether to use gradient checkpointing to save memory
use_checkpoint : false
# Whether to use global batchsize or local batchsize
use_global_batchsize : false

lr_scheduler:
  name : 'cosine'
# Epoch interval to decay LR, used in StepLRScheduler
  decay_epochs : 30
# LR decay rate, used in StepLRScheduler
  decay_rate : 0.1

optimizer:
  name : 'adamw'
# Optimizer Epsilon
  eps : 1e-8
# Optimizer Betas
  betas : [0.9, 0.999]
# SGD momentum
  momentum : 0.9

larc:
  enable: true
  coefficient: 0.02
